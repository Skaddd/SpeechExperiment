---
model_id: openai/whisper-tiny
whisper_training:
  lora_config:
    bias: none
    lora_alpha: 32
    lora_dropout: 0.05
    r: 16
    target_modules: ["q_proj", "v_proj"]
  training_args:
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 1
    learning_rate: 1e-3
    warmup_steps: 50
    num_train_epochs: 1
    evaluation_strategy: "steps"
    fp16: True
    generation_max_length: 128
    logging_steps: 100
    output_dir: "results/"
